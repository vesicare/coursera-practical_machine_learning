---
title: "Practical Machine Learning: the Weight Lifting Exercise Dataset"
author: "ken"
date: "3/2/2019"
output: html_document
---

###Data are kindly provided at: http://groupware.les.inf.puc-rio.br/har. The goal was to learn effects of out of sample error and cross-validation and, these effects on prediction outcome. Here 2 models with different partitioning were compared. Prediction/classification itself did not differ between 2 different cross-validation measures however, probability values differed.  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
train <- readRDS("trainData.RDs")
test <- readRDS("testData.RDs")
suppressMessages(library(tidyverse));
suppressMessages(library(caret));
suppressMessages(library(doParallel))
```

## Look into training data
  Many varibles have NA/missing values. These varibles are deleted because imputation has not been easy for me.    
```{r look into train}
train <- readRDS("trainData.RDs")
train$cvtd_timestamp <- lubridate::dmy_hm(train$cvtd_timestamp)
no_na <- apply(train, 2, function(x) mean(is.na(x))) %>% 
    .[. == 0] %>% 
    names()
```

## Check if the above varibles are without NA in train and test datasets 
```{r look into test, include=TRUE}
test <- readRDS("testData.RDs")
test$cvtd_timestamp <- lubridate::dmy_hm(test$cvtd_timestamp)
apply(test[no_na[no_na !="classe"]], 2, function(x) mean(is.na(x))) %>% 
    .[. == 0] %>% 
    names() %>%
    identical(., no_na[no_na !="classe"])
```
##Modify datasets  
  Change the train- and test-datasets to have only the selected variables.
```{r mod, include=TRUE, echo=FALSE}
train <- train[no_na]
test <- test[no_na[no_na !="classe"]]
train$classe <- as.factor(train$classe)
```

##Prediction methods
  Random forest was chosen because the data have both types of numerics and characters. Also the user_name were omitted. The 'mtry' of randomforest is fixed to 4 to limit the power of prediction. 
##Cross Validation
To learn the effect of Cross Validation/Out of Sample Error, the train data was split into 2 and 5 partitions. Results of "classe" prediction and its probability estimates are described.  

##Model Fitting
####1. Train datasets in 2-fold Cross Validation  
```{r 2 fold, echo=TRUE}
grid <- expand.grid(mtry=4)
cl <- makeCluster(detectCores()-2)
registerDoParallel(cl)
set.seed(999)
rf2.model <- train(
                  classe~., method="rf", data=train[, -c(1:2)],
                  trControl = trainControl(method = "cv",
                                           number = 2,
                                           allowParallel = T),
                  tuneGrid=grid
                  )
stopCluster(cl)
```

####2. Train datasets in 5-fold Cross Validation
```{r 5 fold, echo=TRUE}
grid <- expand.grid(mtry=4)
cl <- makeCluster(detectCores()-2)
registerDoParallel(cl)
set.seed(999)
rf5.model <- train(
                  classe~., method="rf", data=train[, -c(1:2)],
                  trControl = trainControl(method = "cv",
                                           number = 5,
                                           allowParallel = T),
                  tuneGrid=grid
                  )
stopCluster(cl)
```

###Apply the two models to test dataset  
####1. Classification of 'classe'    
Predictions by both models agreed well. I did not see the value of Cross Validation here.  
```{r prediction, echo=TRUE}
table(
   predict(rf2.model, test, type = "raw"),
   predict(rf5.model, test, type = "raw")
)
```

####2. Probablility of the Classification  
However, probability values were not identical between 2 models. 
```{r classification identity, include=TRUE}
identical(predict(rf2.model, test, type = "prob"),
predict(rf5.model, test, type = "prob"))

```

####3. Differences in Probability  
The difference in "prob" values.   
```{r diff prob, include=TRUE}
predict(rf2.model, test, type = "prob") - 
    predict(rf5.model, test, type = "prob")
```

####                ------------ EOF ---------------
